{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "1. What is K-Nearest Neighbors (KNN) and how does it work in both\n",
        "classification and regression problems?\n",
        "\n",
        "ANS-\n",
        "\n",
        "Here’s a detailed explanation of **K-Nearest Neighbors (KNN)** and how it works for **both classification and regression**:\n",
        "\n",
        "---\n",
        "\n",
        "## **1. What is K-Nearest Neighbors (KNN)?**\n",
        "\n",
        "* **KNN** is a **non-parametric, instance-based (lazy) learning algorithm**.\n",
        "* **Non-parametric:** It does not make assumptions about the underlying data distribution.\n",
        "* **Instance-based:** It does not learn a model during training; instead, it stores the training data and makes predictions only when a new sample is encountered.\n",
        "\n",
        "**Key idea:**\n",
        "\n",
        "> A new data point is classified or predicted based on the **“K” most similar (nearest) points** in the training dataset.\n",
        "\n",
        "---\n",
        "\n",
        "## **2. How KNN Works**\n",
        "\n",
        "**Step 1: Choose K**\n",
        "\n",
        "* K = number of nearest neighbors to consider.\n",
        "* Small K → sensitive to noise (high variance)\n",
        "* Large K → smoother predictions, may miss local patterns (high bias)\n",
        "\n",
        "**Step 2: Compute distance**\n",
        "\n",
        "* Common metrics:\n",
        "\n",
        "  * Euclidean distance (most common)\n",
        "  * Manhattan distance\n",
        "  * Minkowski distance\n",
        "* Calculate the distance between the new sample and all training points.\n",
        "\n",
        "**Step 3: Find K nearest neighbors**\n",
        "\n",
        "* Sort all training points by distance.\n",
        "* Select the K closest points.\n",
        "\n",
        "**Step 4: Make a prediction**\n",
        "\n",
        "* **Classification:**\n",
        "\n",
        "  * Assign the class that appears **most frequently** among the K neighbors (majority vote).\n",
        "  * Example: K = 5 neighbors → 3 “Yes”, 2 “No” → predict “Yes”.\n",
        "* **Regression:**\n",
        "\n",
        "  * Take the **average (or weighted average)** of the target values of the K neighbors.\n",
        "  * Example: K = 3 neighbors → target values = [100, 120, 110] → predicted value = 110.\n",
        "\n",
        "---\n",
        "\n",
        "## **3. Advantages of KNN**\n",
        "\n",
        "* Simple and intuitive.\n",
        "* Non-parametric → works for complex decision boundaries.\n",
        "* Naturally handles multi-class classification.\n",
        "\n",
        "---\n",
        "\n",
        "## **4. Disadvantages of KNN**\n",
        "\n",
        "* Computationally expensive for large datasets (distance calculation to all points).\n",
        "* Sensitive to **irrelevant features** and **feature scaling** → usually requires normalization or standardization.\n",
        "* Choice of K and distance metric greatly affects performance.\n",
        "\n",
        "---\n",
        "\n",
        "## **5. Summary Table**\n",
        "\n",
        "| Aspect     | Classification                     | Regression                                   |\n",
        "| ---------- | ---------------------------------- | -------------------------------------------- |\n",
        "| Prediction | Majority vote of K neighbors       | Average (or weighted) of K neighbors’ values |\n",
        "| Output     | Discrete class label               | Continuous numeric value                     |\n",
        "| Example    | Predict if a customer will default | Predict house price                          |\n",
        "| Key metric | Accuracy, F1-score                 | Mean Squared Error, R²                       |\n",
        "\n",
        "---\n",
        "\n",
        "### **6. One-Line Explanation**\n",
        "\n",
        "> KNN predicts the outcome of a new data point based on the labels or values of its K nearest neighbors in the training set, using a distance metric to measure similarity.\n",
        "\n"
      ],
      "metadata": {
        "id": "vMHOUU3JV1Rq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "2. What is the Curse of Dimensionality and how does it affect KNN\n",
        "performance?\n",
        "\n",
        "ANS-\n",
        "\n",
        "Here’s a detailed explanation of the **Curse of Dimensionality** and its impact on **K-Nearest Neighbors (KNN)**:\n",
        "\n",
        "---\n",
        "\n",
        "## **1. What is the Curse of Dimensionality?**\n",
        "\n",
        "The **Curse of Dimensionality** refers to the problems that arise when working with **high-dimensional data** (many features).\n",
        "\n",
        "Key points:\n",
        "\n",
        "1. **Data sparsity:**\n",
        "\n",
        "   * As the number of dimensions increases, the volume of the feature space grows **exponentially**.\n",
        "   * Data points become **sparse**, meaning neighbors are far apart, and “closeness” loses meaning.\n",
        "\n",
        "2. **Distance measures become less meaningful:**\n",
        "\n",
        "   * Most distance metrics (like Euclidean distance) become **less discriminative** in high dimensions.\n",
        "   * The difference between the nearest and farthest points becomes very small.\n",
        "\n",
        "3. **Increased computational cost:**\n",
        "\n",
        "   * More dimensions → more calculations for distance → slower predictions, especially for KNN.\n",
        "\n",
        "---\n",
        "\n",
        "## **2. How it Affects KNN Performance**\n",
        "\n",
        "KNN relies on the assumption that **similar points are close in the feature space**.\n",
        "\n",
        "**Problems in high dimensions:**\n",
        "\n",
        "1. **Neighbors are far away:**\n",
        "\n",
        "   * The “nearest neighbor” may be almost as far as the farthest point → weakens the predictive power.\n",
        "\n",
        "2. **Noise sensitivity:**\n",
        "\n",
        "   * High-dimensional data often includes irrelevant or redundant features → KNN may consider noisy dimensions, degrading accuracy.\n",
        "\n",
        "3. **Need for more data:**\n",
        "\n",
        "   * To maintain the same density of points in higher dimensions, you need exponentially more training data.\n",
        "   * Otherwise, KNN struggles to find meaningful neighbors.\n",
        "\n",
        "---\n",
        "\n",
        "## **3. Mitigation Strategies**\n",
        "\n",
        "1. **Feature selection:**\n",
        "\n",
        "   * Keep only the most relevant features to reduce dimensionality.\n",
        "\n",
        "2. **Dimensionality reduction:**\n",
        "\n",
        "   * Techniques like **PCA (Principal Component Analysis)**, **t-SNE**, or **UMAP** can reduce dimensions while preserving structure.\n",
        "\n",
        "3. **Distance metric adjustment:**\n",
        "\n",
        "   * Use **weighted distances** or **Mahalanobis distance** that account for feature relevance.\n",
        "\n",
        "4. **Increase dataset size:**\n",
        "\n",
        "   * More data points help maintain meaningful neighborhoods in higher dimensions.\n",
        "\n",
        "---\n",
        "\n",
        "### **4. Intuition / Analogy**\n",
        "\n",
        "> Imagine trying to find the closest friend in a huge city.\n",
        "> In 2D (like a small town), your closest friend is easy to locate.\n",
        "> In 100 dimensions (like hundreds of features), everyone seems equally far away, making it hard to define “nearest.”\n",
        "\n",
        "---\n",
        "\n",
        "### **5. One-Line Exam Answer**\n",
        "\n",
        "> The Curse of Dimensionality occurs when high-dimensional data makes distance metrics less meaningful and data sparse, causing KNN to struggle to identify truly “near” neighbors and degrading its performance.\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "sUVR9qmBWIPs"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "3. What is Principal Component Analysis (PCA)? How is it different from\n",
        "feature selection?\n",
        "\n",
        "ANS-\n",
        "\n",
        "Here’s a clear explanation of **Principal Component Analysis (PCA)** and how it differs from **feature selection**:\n",
        "\n",
        "---\n",
        "\n",
        "## **1. What is PCA?**\n",
        "\n",
        "**Principal Component Analysis (PCA)** is a **dimensionality reduction technique** used to transform high-dimensional data into a **lower-dimensional space** while retaining as much **variance (information)** as possible.\n",
        "\n",
        "**Key points:**\n",
        "\n",
        "* PCA creates **new features** called **principal components (PCs)**.\n",
        "* Each principal component is a **linear combination of the original features**.\n",
        "* The components are **orthogonal (uncorrelated)** to each other.\n",
        "* The first principal component captures the **most variance**, the second captures the next most, and so on.\n",
        "\n",
        "**Steps in PCA:**\n",
        "\n",
        "1. Standardize the data (mean=0, variance=1).\n",
        "2. Compute the covariance matrix.\n",
        "3. Compute eigenvectors and eigenvalues of the covariance matrix.\n",
        "4. Sort eigenvectors by eigenvalues (largest first) → these are the principal components.\n",
        "5. Project the original data onto the top **k components** to reduce dimensionality.\n",
        "\n",
        "**Use cases:**\n",
        "\n",
        "* Reduce dimensionality to speed up training.\n",
        "* Remove multicollinearity in features.\n",
        "* Visualize high-dimensional data in 2D or 3D.\n",
        "\n",
        "---\n",
        "\n",
        "## **2. How PCA Works vs Feature Selection**\n",
        "\n",
        "| Aspect                       | PCA (Feature Extraction)                                                | Feature Selection                                            |\n",
        "| ---------------------------- | ----------------------------------------------------------------------- | ------------------------------------------------------------ |\n",
        "| **Goal**                     | Create new features (principal components) that summarize original data | Select a subset of original features                         |\n",
        "| **Output**                   | Linear combinations of original features                                | Original features only                                       |\n",
        "| **Variance Preservation**    | Maximizes variance in new components                                    | Doesn’t change features, may or may not capture all variance |\n",
        "| **Dimensionality Reduction** | Always reduces dimensions                                               | Reduces by selecting fewer features                          |\n",
        "| **Correlation Handling**     | Removes correlation among components                                    | Correlated features may remain                               |\n",
        "| **Interpretability**         | Harder to interpret (components are combinations)                       | Easier to interpret (original features kept)                 |\n",
        "\n",
        "---\n",
        "\n",
        "## **3. Intuition / Analogy**\n",
        "\n",
        "> PCA is like **rotating and projecting a cloud of points** in a high-dimensional space onto directions where the points are most spread out, keeping the “essence” of the data.\n",
        "> Feature selection is like **choosing the most important existing axes** without creating new ones.\n",
        "\n",
        "---\n",
        "\n",
        "### **4. One-Line Exam Answer**\n",
        "\n",
        "> PCA reduces dimensionality by creating new orthogonal features (principal components) that capture maximum variance, whereas feature selection reduces dimensionality by picking a subset of the original features without transformation.\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "oAferC8bWfAi"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "4. What are eigenvalues and eigenvectors in PCA, and why are they\n",
        "important?\n",
        "\n",
        "ANS-\n",
        "\n",
        "Here’s a clear explanation of **eigenvalues and eigenvectors in PCA** and why they are important:\n",
        "\n",
        "---\n",
        "\n",
        "## **1. Eigenvectors and Eigenvalues: The Basics**\n",
        "\n",
        "In linear algebra, for a square matrix (A):\n",
        "\n",
        "[\n",
        "A \\mathbf{v} = \\lambda \\mathbf{v}\n",
        "]\n",
        "\n",
        "* **(\\mathbf{v})** → **eigenvector**: a direction in space that remains pointing in the same direction after transformation by (A).\n",
        "* **(\\lambda)** → **eigenvalue**: the scaling factor by which the eigenvector is stretched or shrunk.\n",
        "\n",
        "Intuitively:\n",
        "\n",
        "* Eigenvectors = directions of data variation\n",
        "* Eigenvalues = magnitude of variation along those directions\n",
        "\n",
        "---\n",
        "\n",
        "## **2. How They Are Used in PCA**\n",
        "\n",
        "**Step 1: Compute covariance matrix**\n",
        "\n",
        "* Let (X) be your standardized data ((mean=0)).\n",
        "* Covariance matrix (C = \\frac{1}{n-1} X^T X) captures how features vary together.\n",
        "\n",
        "**Step 2: Find eigenvectors and eigenvalues of the covariance matrix**\n",
        "\n",
        "* **Eigenvectors** → directions along which data varies the most (principal components).\n",
        "* **Eigenvalues** → variance explained by each eigenvector.\n",
        "\n",
        "**Step 3: Sort eigenvectors by eigenvalues**\n",
        "\n",
        "* Higher eigenvalue → more variance explained → more important component.\n",
        "\n",
        "**Step 4: Project data onto top eigenvectors**\n",
        "\n",
        "* Reduces dimensionality while keeping most information.\n",
        "\n",
        "---\n",
        "\n",
        "## **3. Why They Are Important**\n",
        "\n",
        "1. **Eigenvectors = principal directions**\n",
        "\n",
        "   * They define the new axes (principal components) along which the data is spread.\n",
        "\n",
        "2. **Eigenvalues = measure of importance**\n",
        "\n",
        "   * Larger eigenvalues mean the component captures more variance.\n",
        "   * Helps decide how many components to keep (e.g., choose top K that explain 95% variance).\n",
        "\n",
        "3. **Dimensionality reduction**\n",
        "\n",
        "   * Eigenvectors allow us to transform the data into fewer dimensions without losing significant information.\n",
        "\n",
        "---\n",
        "\n",
        "### **4. Intuition / Analogy**\n",
        "\n",
        "> Imagine a cloud of 2D points shaped like an ellipse:\n",
        ">\n",
        "> * The **long axis** is the direction of maximum variance → **first eigenvector**, eigenvalue = length along that axis.\n",
        "> * The **short axis** is the second eigenvector, eigenvalue = length along that axis.\n",
        ">   PCA projects the data onto these axes to reduce dimensions while keeping the spread.\n",
        "\n",
        "---\n",
        "\n",
        "### **5. One-Line Exam Answer**\n",
        "\n",
        "> In PCA, eigenvectors define the principal directions of data variance, and eigenvalues indicate how much variance each direction captures, guiding which components to retain for dimensionality reduction.\n",
        "\n"
      ],
      "metadata": {
        "id": "JQ6fSxtnXZcw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "5.  How do KNN and PCA complement each other when applied in a single\n",
        "pipeline?\n",
        "\n",
        "ANS-\n",
        "\n",
        "Here’s a clear explanation of how **KNN and PCA** can complement each other when used together in a single pipeline:\n",
        "\n",
        "---\n",
        "\n",
        "## **1. The Problem**\n",
        "\n",
        "* **K-Nearest Neighbors (KNN)** relies on **distance metrics** (usually Euclidean distance) to find neighbors.\n",
        "* **Curse of Dimensionality:** In high-dimensional datasets, distances become less meaningful, and KNN performance drops.\n",
        "* **High-dimensional data** may contain redundant or irrelevant features that confuse KNN.\n",
        "\n",
        "---\n",
        "\n",
        "## **2. How PCA Helps**\n",
        "\n",
        "**Principal Component Analysis (PCA)** reduces dimensionality while retaining the **most variance in the data**.\n",
        "\n",
        "* **Key idea:** Project high-dimensional data onto a lower-dimensional space that captures the main patterns.\n",
        "* **Benefits for KNN:**\n",
        "\n",
        "  1. Reduces noise and irrelevant features → more meaningful distances.\n",
        "  2. Reduces computational cost → fewer dimensions to calculate distances.\n",
        "  3. Mitigates the curse of dimensionality → KNN neighbors are more informative.\n",
        "\n",
        "---\n",
        "\n",
        "## **3. How They Work Together**\n",
        "\n",
        "**Pipeline:**\n",
        "\n",
        "1. **Step 1:** Apply PCA to transform the original features into top principal components.\n",
        "2. **Step 2:** Feed the lower-dimensional data into KNN.\n",
        "3. **Step 3:** KNN computes distances in the PCA-transformed space and predicts labels (classification) or values (regression).\n",
        "\n",
        "**Intuition:**\n",
        "\n",
        "> PCA acts as a **preprocessing step** that summarizes the most important information, while KNN uses that information to make more reliable predictions.\n",
        "\n",
        "---\n",
        "\n",
        "## **4. Advantages of the Pipeline**\n",
        "\n",
        "| Benefit             | Explanation                                                                            |\n",
        "| ------------------- | -------------------------------------------------------------------------------------- |\n",
        "| Faster computation  | Fewer dimensions → less distance calculations                                          |\n",
        "| Better accuracy     | Irrelevant/noisy features are removed, improving neighbor selection                    |\n",
        "| Reduced overfitting | KNN is less likely to overfit noisy high-dimensional data                              |\n",
        "| Visualization       | PCA components can be plotted in 2D/3D to see data clusters, aiding KNN interpretation |\n",
        "\n",
        "---\n",
        "\n",
        "### **5. One-Line Summary**\n",
        "\n",
        "> Using PCA before KNN reduces dimensionality and noise, making distances more meaningful, improving KNN accuracy, and reducing computation time.\n",
        "\n"
      ],
      "metadata": {
        "id": "Kpdelm5gXwC4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "6.  Train a KNN Classifier on the Wine dataset with and without feature\n",
        "scaling. Compare model accuracy in both cases.\n",
        "\n",
        "ANS-\n",
        "\n",
        "# Import required libraries\n",
        "from sklearn.datasets import load_wine\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Load the Wine dataset\n",
        "data = load_wine()\n",
        "X = data.data\n",
        "y = data.target\n",
        "\n",
        "# Split dataset into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.3, random_state=42\n",
        ")\n",
        "\n",
        "# -------------------------------\n",
        "# 1. KNN WITHOUT feature scaling\n",
        "# -------------------------------\n",
        "knn = KNeighborsClassifier(n_neighbors=5)\n",
        "knn.fit(X_train, y_train)\n",
        "y_pred_no_scaling = knn.predict(X_test)\n",
        "accuracy_no_scaling = accuracy_score(y_test, y_pred_no_scaling)\n",
        "\n",
        "# -------------------------------\n",
        "# 2. KNN WITH feature scaling\n",
        "# -------------------------------\n",
        "scaler = StandardScaler()\n",
        "X_train_scaled = scaler.fit_transform(X_train)\n",
        "X_test_scaled = scaler.transform(X_test)\n",
        "\n",
        "knn_scaled = KNeighborsClassifier(n_neighbors=5)\n",
        "knn_scaled.fit(X_train_scaled, y_train)\n",
        "y_pred_scaled = knn_scaled.predict(X_test_scaled)\n",
        "accuracy_scaled = accuracy_score(y_test, y_pred_scaled)\n",
        "\n",
        "# -------------------------------\n",
        "# Print Results\n",
        "# -------------------------------\n",
        "print\n"
      ],
      "metadata": {
        "id": "IgwsnBQbX3t3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "7. Train a PCA model on the Wine dataset and print the explained variance\n",
        "ratio of each principal component.\n",
        "\n",
        "\n",
        "ANS-\n",
        "\n",
        "# Import required libraries\n",
        "from sklearn.datasets import load_wine\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.decomposition import PCA\n",
        "\n",
        "# Load Wine dataset\n",
        "data = load_wine()\n",
        "X = data.data\n",
        "y = data.target\n",
        "\n",
        "# Standardize the features\n",
        "scaler = StandardScaler()\n",
        "X_scaled = scaler.fit_transform(X)\n",
        "\n",
        "# Train PCA\n",
        "pca = PCA()\n",
        "X_pca = pca.fit_transform(X_scaled)\n",
        "\n",
        "# Print explained variance ratio for each principal component\n",
        "explained_variance = pca.explained_variance_ratio_\n",
        "for i, ratio in enumerate(explained_variance):\n",
        "    print(f\"Principal Component {i+1}: {ratio:.4f}\")\n"
      ],
      "metadata": {
        "id": "OVOOok-JYChY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "8. Train a KNN Classifier on the PCA-transformed dataset (retain top 2 components). Compare the accuracy with the original dataset.\n",
        "\n",
        "ANS-\n",
        "\n",
        "# Import required libraries\n",
        "from sklearn.datasets import load_wine\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Load Wine dataset\n",
        "data = load_wine()\n",
        "X = data.data\n",
        "y = data.target\n",
        "\n",
        "# Split original dataset\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.3, random_state=42\n",
        ")\n",
        "\n",
        "# -------------------------------\n",
        "# 1. KNN on ORIGINAL dataset\n",
        "# -------------------------------\n",
        "scaler = StandardScaler()\n",
        "X_train_scaled = scaler.fit_transform(X_train)\n",
        "X_test_scaled = scaler.transform(X_test)\n",
        "\n",
        "knn_original = KNeighborsClassifier(n_neighbors=5)\n",
        "knn_original.fit(X_train_scaled, y_train)\n",
        "y_pred_original = knn_original.predict(X_test_scaled)\n",
        "accuracy_original = accuracy_score(y_test, y_pred_original)\n",
        "\n",
        "# -------------------------------\n",
        "# 2. KNN on PCA-transformed dataset (top 2 components)\n",
        "# -------------------------------\n",
        "pca = PCA(n_components=2)\n",
        "X_train_pca = pca.fit_transform(X_train_scaled)\n",
        "X_test_pca = pca.transform(X_test_scaled)\n",
        "\n",
        "knn_pca = KNeighborsClassifier(n_neighbors=5)\n",
        "knn_pca.fit(X_train_pca, y_train)\n",
        "y_pred_pca = knn_pca.predict(X_test_pca)\n",
        "accuracy_pca = accuracy_score(y_test, y_pred_pca)\n",
        "\n",
        "# -------------------------------\n",
        "# Print results\n",
        "# -------------------------------\n",
        "print(\"KNN Accuracy on Original Dataset:\", accuracy_original)\n",
        "print(\"KNN Accuracy on PCA-transformed Dataset (2 components):\", accuracy_pca)\n"
      ],
      "metadata": {
        "id": "tBh-b6kWYN1h"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "9. Train a KNN Classifier with different distance metrics (euclidean,\n",
        "manhattan) on the scaled Wine dataset and compare the results.\n",
        "\n",
        "ANS-\n",
        "# Import required libraries\n",
        "from sklearn.datasets import load_wine\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Load Wine dataset\n",
        "data = load_wine()\n",
        "X = data.data\n",
        "y = data.target\n",
        "\n",
        "# Split dataset into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.3, random_state=42\n",
        ")\n",
        "\n",
        "# Scale features\n",
        "scaler = StandardScaler()\n",
        "X_train_scaled = scaler.fit_transform(X_train)\n",
        "X_test_scaled = scaler.transform(X_test)\n",
        "\n",
        "# -------------------------------\n",
        "# KNN with Euclidean distance\n",
        "# -------------------------------\n",
        "knn_euclidean = KNeighborsClassifier(n_neighbors=5, metric='euclidean')\n",
        "knn_euclidean.fit(X_train_scaled, y_train)\n",
        "y_pred_euclidean = knn_euclidean.predict(X_test_scaled)\n",
        "accuracy_euclidean = accuracy_score(y_test, y_pred_euclidean)\n",
        "\n",
        "# -------------------------------\n",
        "# KNN with Manhattan distance\n",
        "# -------------------------------\n",
        "knn_manhattan = KNeighborsClassifier(n_neighbors=5, metric='manhattan')\n",
        "knn_manhattan.fit(X_train_scaled, y_train)\n",
        "y_pred_manhattan = knn_manhattan.predict(X_test_scaled)\n",
        "accuracy_manhattan = accuracy_score(y_test, y_pred_manhattan)\n",
        "\n",
        "# -------------------------------\n",
        "# Print results\n",
        "# -------------------------------\n",
        "print(\"KNN Accuracy with Euclidean distance:\", accuracy_euclidean)\n",
        "print(\"KNN Accuracy with Manhattan distance:\", accuracy_manhattan)\n"
      ],
      "metadata": {
        "id": "1uXAXnlnYbUK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "10. You are working with a high-dimensional gene expression dataset to\n",
        "classify patients with different types of cancer.\n",
        "Due to the large number of features and a small number of samples, traditional models\n",
        "overfit.\n",
        "Explain how you would:\n",
        "● Use PCA to reduce dimensionality\n",
        "● Decide how many components to keep\n",
        "● Use KNN for classification post-dimensionality reduction\n",
        "● Evaluate the model\n",
        "● Justify this pipeline to your stakeholders as a robust solution for real-world\n",
        "biomedical data\n",
        "\n",
        "ANS-\n",
        "\n",
        "# Import required libraries\n",
        "from sklearn.datasets import load_wine\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Load Wine dataset\n",
        "data = load_wine()\n",
        "X = data.data\n",
        "y = data.target\n",
        "\n",
        "# Split dataset into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.3, random_state=42\n",
        ")\n",
        "\n",
        "# Scale features\n",
        "scaler = StandardScaler()\n",
        "X_train_scaled = scaler.fit_transform(X_train)\n",
        "X_test_scaled = scaler.transform(X_test)\n",
        "\n",
        "# -------------------------------\n",
        "# KNN with Euclidean distance\n",
        "# -------------------------------\n",
        "knn_euclidean = KNeighborsClassifier(n_neighbors=5, metric='euclidean')\n",
        "knn_euclidean.fit(X_train_scaled, y_train)\n",
        "y_pred_euclidean = knn_euclidean.predict(X_test_scaled)\n",
        "accuracy_euclidean = accuracy_score(y_test, y_pred_euclidean)\n",
        "\n",
        "# -------------------------------\n",
        "# KNN with Manhattan distance\n",
        "# -------------------------------\n",
        "knn_manhattan = KNeighborsClassifier(n_neighbors=5, metric='manhattan')\n",
        "knn_manhattan.fit(X_train_scaled, y_train)\n",
        "y_pred_manhattan = knn_manhattan.predict(X_test_scaled)\n",
        "accuracy_manhattan = accuracy_score(y_test, y_pred_manhattan)\n",
        "\n",
        "# -------------------------------\n",
        "# Print results\n",
        "# -------------------------------\n",
        "print(\"KNN Accuracy with Euclidean distance:\", accuracy_euclidean)\n",
        "print(\"KNN Accuracy with Manhattan distance:\", accuracy_manhattan)\n"
      ],
      "metadata": {
        "id": "ldqIHQv-Youg"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}